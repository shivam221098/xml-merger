# -*- coding: utf-8 -*-
"""Movie Scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vjUJpx-dNlFnr8SbK9fhDxZwIdOno08N
"""

import pandas as pd
import requests
import numpy as np
from bs4 import BeautifulSoup
import re


# %%
countl = 0


def get_director_genre_production(link):
    global countl
    dict_data = {}

    global session
    content = session.get(link)

    table_soup = BeautifulSoup(content.content, "html.parser")

    tables = table_soup.find("div", attrs={"id": "summary"})
    # print(table)
    rows = (tables.find_all("table"))
    director_name = None
    try:
        try:
            cast_crew = link.replace("=summary", "=cast-and-crew")
            cast_tables = session.get(cast_crew)

            cast_soup = BeautifulSoup(cast_tables.content, "html.parser")

            cast_div = cast_soup.find_all("div", {"class": "cast_new"})
            cast_table = cast_div[2].find("table")
            director = cast_table.find_all("tr")[0].text.split("\n")
            index = director.index("Director")
            director_name = director[index - 2]
            print(cast_crew)
        except ValueError:
            director_name = None

        for tr in rows[4].find_all("tr"):
            td = tr.text.split("\n")
            if any(["Genre" in data for data in td]):
                dict_data["Genre"] = " ".join(td).split(":")[-1]
            elif any(["Production Companies" in data for data in td]):
                dict_data["Production Company"] = " ".join(td).split(":")[-1]
    except IndexError:
        countl += 1
        return {
            "Production Company": None,
            "Genre": None,
            "Movie name": get_movie_name(link),
            "Director name": director_name
        }

        # elif any([""])
    dict_data.update({"Movie name": get_movie_name(link)})
    dict_data.update({"Director name": director_name})
    countl += 1
    print(f"LINKS scraped {countl}", end="\r")
    return dict_data


# %%

def get_movie_name(link):
    pattern = re.compile("/[A-Za-z0-9-()]+#")
    x = re.search(pattern, link)
    if x is not None:
        start = x.start() + 1
        stop = x.end() - 1

        return link[start:stop]


# %%

import concurrent.futures


def process_parallel(links):
    with concurrent.futures.ThreadPoolExecutor() as executor:
        data = executor.map(get_director_genre_production, links)

    return data


# %%

url_range = list(range(101, 1000, 100))
# print(url_range)
REQUIRED_ROWS = 1000

session = requests.Session()
# print(soup)

dataframes = []
count = 1
hrefs = []
genre_director_company = []

while count < REQUIRED_ROWS:
    links = []
    url = 'https://www.the-numbers.com/box-office-records/worldwide/all-movies/cumulative/all-time/' + str(count)
    print(url)

    response = session.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    for item in soup.find_all('tr'):
        link = item.find_all('a')
        links.append(link)

        if len(link) > 1:
            hrefs.append("https://www.the-numbers.com" + link[-1]["href"])

    list_check = []
    for tr in soup.find_all('tr'):
        list_check.append(tr.get_text().split('\n'))

    array = np.array(list_check)
    array = array.reshape(101, 8)
    # print(np.size(array))

    df = pd.DataFrame(array, index=array[:, 1])
    new_header = df.iloc[0]
    df = df[1:]
    df.columns = new_header
    df.columns = (
        ['Rank', 'Rank', 'Year', 'Movie', 'WorldwideBox_Office', 'DomesticBox_Office', 'InternationalBox_Office',
         'Extra'])
    dataframes.append(df)
    # print(len(df))

    count += 100

# print(links[1][1])
new_df = pd.concat(dataframes)
new_df.to_csv("test_data.csv", index=False)

genre_director_company = process_parallel(hrefs)
other_details_df = pd.DataFrame(genre_director_company)
other_details_df.to_csv("director_genre_company.csv")

exit(0)

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# %matplotlib inline
no_sign_WW = []
no_sign_intl = []
no_sign_dom = []
index = 0
for item in df.WorldwideBox_Office:
    no_sign_WW.append(int(float(item[1:].replace(',', ''))))
    index += 1

index = 0
for item in df.DomesticBox_Office:
    number = item[1:].replace(',', '')
    # print(number)
'''    no_sign_dom.append(int(float(number)))
    index += 1'''

index = 0
for item in df.InternationalBox_Office:
    no_sign_intl.append(int(float(item[1:].replace(',', ''))))
    index += 1

df['no_sign_ww'] = no_sign_WW
df['no_sign_intl'] = no_sign_intl
# df['no_sign_dom'] = no_sign_dom

# print(df.no_sign_ww)
df = df.sort_values('Year')

plt.figure(figsize=(20, 10))
fig = plt.scatter(df.Year, df.no_sign_ww / 10000000)
# fig2 = plt.scatter(df.Year,df.no_sign_dom/10000000)
fig3 = plt.scatter(df.Year, df.no_sign_intl / 10000000)

plt.figure(figsize=(20, 10))

fig4 = plt.hist(df.Year)

import pandas as pd
import requests
import re
import numpy as np
from bs4 import BeautifulSoup

url_range = list(range(1, 1000, 100))
print(url_range)

url = 'https://www.the-numbers.com/box-office-records/worldwide/all-movies/cumulative/all-time/'
list_check = []
for index in url_range:
    response = requests.get(url + str(index))
    soup = BeautifulSoup(response.text, 'lxml')
    # soup = soup[1:]
    # print(soup)

    # links = soup.select('td.href')

    for tr in soup.find_all('tr'):
        list_check.append(tr.get_text().split('\n'))

array = np.array(list_check)
print(array)

# array = array.reshape(101,8)
'''
df = pd.DataFrame(array,index=array[:,1])
new_header = df.iloc[0]
df = df[1:]
df.columns = new_header
df
'''

# print(np.size(array))
print(array[101])
array = array.reshape(1010, 8)
# 0, 100, 201, 302, 403, 504, 605, 706, 807, 908 are headers
headers = df2.columns
print(headers)

for index in df2.iloc[:]:
    if index == headers:
        df2.iloc[headers].delete()

print(df2.iloc[100])
# for index in url_range[-1::-1]:
# print(index)
# print(df2.iloc[index-1])


df2 = pd.DataFrame(array, index=array[:, 1])
new_header = df2.iloc[0]
df2 = df2[1:]
df2.columns = new_header

# print('this index is' + df2.iloc[100])

url = 'https://www.the-numbers.com/movie/Avatar#tab=cast-and-crew'

response = requests.get(url)
soup2 = BeautifulSoup(response.text, 'lxml')
print(soup2)
